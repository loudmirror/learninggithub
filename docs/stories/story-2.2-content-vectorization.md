# Story 2.2: å†…å®¹å¤„ç†ä¸å‘é‡åŒ–æœåŠ¡

## ğŸ“‹ Story å…ƒä¿¡æ¯

- **Story ID**: STORY-2.2
- **Epic**: MVP v0.1
- **æ‰€å±è¿­ä»£**: è¿­ä»£ 2 - çœŸå®æœåŠ¡é›†æˆ
- **çŠ¶æ€**: Draft
- **ä¼˜å…ˆçº§**: High
- **é¢„ä¼°æ—¶é—´**: 2-3 å¤©
- **è´Ÿè´£äºº**: Dev Agent
- **ä¾èµ–**: STORY-2.1 (GitHub ä»“åº“è§£ææœåŠ¡)

---

## ğŸ“– User Story

**As a** ç³»ç»Ÿå¼€å‘è€…
**I want** å®ç°å†…å®¹æå–ã€åˆ†å—å’Œå‘é‡åŒ–æœåŠ¡
**So that** å¯ä»¥å°† GitHub ä»“åº“å†…å®¹è½¬æ¢ä¸ºå¯æ£€ç´¢çš„å‘é‡æ•°æ®ï¼Œä¸º RAG æ•™ç¨‹ç”Ÿæˆæä¾›åŸºç¡€

---

## ğŸ¯ èƒŒæ™¯ä¸ä¸Šä¸‹æ–‡

### é¡¹ç›®ä¸Šä¸‹æ–‡
åœ¨ Story 2.1 å®Œæˆä»“åº“è§£æåï¼Œéœ€è¦å¯¹å…‹éš†çš„ä»£ç ä»“åº“è¿›è¡Œæ·±åº¦å¤„ç†ï¼š
1. æå–å…³é”®ä»£ç æ–‡ä»¶å†…å®¹ï¼ˆè¿‡æ»¤äºŒè¿›åˆ¶æ–‡ä»¶ã€ä¾èµ–ç›®å½•ç­‰ï¼‰
2. å°†æ–‡ä»¶å†…å®¹æŒ‰è¯­ä¹‰å•å…ƒåˆ†å—ï¼ˆä¿æŒä»£ç å®Œæ•´æ€§ï¼‰
3. ä½¿ç”¨ OpenAI Embedding API ç”Ÿæˆå‘é‡
4. å­˜å‚¨åˆ° ChromaDB å‘é‡æ•°æ®åº“

### æŠ€æœ¯ä¸Šä¸‹æ–‡
- **å‘é‡åŒ–æ¨¡å‹**: OpenAI `text-embedding-ada-002` (1536 ç»´)
- **å‘é‡æ•°æ®åº“**: ChromaDB (embedded mode)
- **åˆ†å—ç­–ç•¥**:
  - å•æ–‡ä»¶ < 2000 tokens: æ•´æ–‡ä»¶ä½œä¸ºä¸€ä¸ª chunk
  - å•æ–‡ä»¶ â‰¥ 2000 tokens: æŒ‰å‡½æ•°/ç±»çº§åˆ«åˆ†å—
- **æ–‡ä»¶è¿‡æ»¤**: ä½¿ç”¨ `.gitignore` è§„åˆ™ + è‡ªå®šä¹‰é»‘åå•
- **å…ƒæ•°æ®**: ä¿ç•™æ–‡ä»¶è·¯å¾„ã€è¯­è¨€ã€ä»£ç èŒƒå›´ç­‰ä¿¡æ¯

### è¿­ä»£ç›®æ ‡
å®ç°ä»ä»“åº“ç›®å½•åˆ°å‘é‡æ•°æ®åº“çš„å®Œæ•´ pipelineï¼Œæ”¯æŒå¢é‡æ›´æ–°å’Œé”™è¯¯é‡è¯•ã€‚

---

## âœ… éªŒæ”¶æ ‡å‡†

### åŠŸèƒ½æ€§éœ€æ±‚

1. **AC-2.2.1**: å†…å®¹æå–æœåŠ¡èƒ½å¤Ÿéå†å…‹éš†çš„ä»“åº“ç›®å½•ï¼Œè¯†åˆ«ä»£ç æ–‡ä»¶
   - æ”¯æŒå¸¸è§ç¼–ç¨‹è¯­è¨€ï¼ˆPython, JS/TS, Java, Go, Rust ç­‰ï¼‰
   - è¿‡æ»¤ `node_modules`, `venv`, `.git`, `dist`, `build` ç­‰ç›®å½•
   - è¿‡æ»¤äºŒè¿›åˆ¶æ–‡ä»¶ã€å›¾ç‰‡ã€è§†é¢‘ç­‰éæ–‡æœ¬å†…å®¹
   - å°Šé‡ `.gitignore` è§„åˆ™

2. **AC-2.2.2**: æ–‡ä»¶å†…å®¹è¯»å–ä½¿ç”¨æ­£ç¡®çš„ç¼–ç æ£€æµ‹
   - é»˜è®¤ UTF-8ï¼Œè‡ªåŠ¨æ£€æµ‹å…¶ä»–ç¼–ç ï¼ˆå¦‚ GBKï¼‰
   - é‡åˆ°æ— æ³•è§£ç çš„æ–‡ä»¶è·³è¿‡å¹¶è®°å½•æ—¥å¿—
   - å•æ–‡ä»¶å¤§å°é™åˆ¶ 1MBï¼ˆå¯é…ç½®ï¼‰

3. **AC-2.2.3**: æ–‡æœ¬åˆ†å—ç­–ç•¥èƒ½å¤Ÿä¿æŒä»£ç è¯­ä¹‰å®Œæ•´æ€§
   - å°æ–‡ä»¶ï¼ˆ< 2000 tokensï¼‰: æ•´æ–‡ä»¶ä½œä¸ºä¸€ä¸ª chunk
   - å¤§æ–‡ä»¶ï¼ˆâ‰¥ 2000 tokensï¼‰: æŒ‰å‡½æ•°/ç±»çº§åˆ«åˆ†å—
   - ä½¿ç”¨ AST è§£æå™¨ï¼ˆå¦‚ `tree-sitter`ï¼‰è¯†åˆ«ä»£ç è¾¹ç•Œ
   - Fallback: ä½¿ç”¨è¡Œå·åˆ†å—ï¼ˆå¦‚ AST è§£æå¤±è´¥ï¼‰

4. **AC-2.2.4**: Embedding ç”Ÿæˆè°ƒç”¨ OpenAI API
   - ä½¿ç”¨ `text-embedding-ada-002` æ¨¡å‹
   - æ‰¹é‡å¤„ç†ï¼ˆæ¯æ‰¹æœ€å¤š 100 ä¸ª chunksï¼‰
   - å®ç° Rate Limiting (3000 RPM)
   - é”™è¯¯é‡è¯•æœºåˆ¶ï¼ˆæŒ‡æ•°é€€é¿ï¼Œæœ€å¤š 3 æ¬¡ï¼‰

5. **AC-2.2.5**: ChromaDB é›†æˆèƒ½å¤Ÿå­˜å‚¨å‘é‡å’Œå…ƒæ•°æ®
   - Collection å‘½åè§„åˆ™: `repo_{owner}_{name}`
   - æ¯ä¸ª chunk åŒ…å«å…ƒæ•°æ®:
     ```python
     {
       "file_path": "src/index.ts",
       "language": "typescript",
       "chunk_type": "function",  # file | function | class
       "start_line": 10,
       "end_line": 25,
       "chunk_index": 0
     }
     ```
   - Document ID æ ¼å¼: `{file_path}::{chunk_index}`
   - æ”¯æŒå¹‚ç­‰æ€§æ›´æ–°ï¼ˆç›¸åŒ ID è¦†ç›–ï¼‰

6. **AC-2.2.6**: æä¾›å‘é‡åŒ–ä»»åŠ¡çŠ¶æ€æŸ¥è¯¢æ¥å£
   - çŠ¶æ€: `pending`, `processing`, `completed`, `failed`
   - è¿”å›è¿›åº¦ä¿¡æ¯: å·²å¤„ç†æ–‡ä»¶æ•° / æ€»æ–‡ä»¶æ•°
   - è¿”å›é”™è¯¯è¯¦æƒ…ï¼ˆå¦‚æœå¤±è´¥ï¼‰

7. **AC-2.2.7**: API ç«¯ç‚¹è®¾è®¡ç¬¦åˆè§„èŒƒ
   ```
   POST /api/vectorize
   Body: { "repo_url": "https://github.com/..." }
   Response: { "ok": true, "data": { "task_id": "...", "status": "processing" } }

   GET /api/vectorize/{task_id}
   Response: { "ok": true, "data": { "status": "completed", "stats": {...} } }
   ```

8. **AC-2.2.8**: Token è®¡æ•°å‡†ç¡®
   - ä½¿ç”¨ `tiktoken` åº“è®¡ç®— token æ•°
   - ä½¿ç”¨ `cl100k_base` encoderï¼ˆåŒ¹é… ada-002ï¼‰
   - å•ä¸ª chunk ä¸è¶…è¿‡ 8191 tokensï¼ˆæ¨¡å‹é™åˆ¶ï¼‰

### è´¨é‡éœ€æ±‚

9. **AC-2.2.9**: é”™è¯¯å¤„ç†å®Œå–„
   - OpenAI API é”™è¯¯: Rate limit, timeout, quota exceeded
   - æ–‡ä»¶è¯»å–é”™è¯¯: ç¼–ç é”™è¯¯ã€æƒé™é”™è¯¯ã€æ–‡ä»¶ä¸å­˜åœ¨
   - ChromaDB é”™è¯¯: è¿æ¥å¤±è´¥ã€å†™å…¥å†²çª
   - æ‰€æœ‰é”™è¯¯è®°å½•ç»“æ„åŒ–æ—¥å¿—ï¼ŒåŒ…å«ä¸Šä¸‹æ–‡ä¿¡æ¯

10. **AC-2.2.10**: æ€§èƒ½æ»¡è¶³è¦æ±‚
    - å¤„ç† 1000 ä¸ªæ–‡ä»¶ï¼ˆçº¦ 50MB ä»£ç ï¼‰â‰¤ 5 åˆ†é’Ÿ
    - å†…å­˜å ç”¨ â‰¤ 512MBï¼ˆæµå¼å¤„ç†å¤§æ–‡ä»¶ï¼‰
    - OpenAI API å¹¶å‘æ§åˆ¶ï¼ˆé¿å… Rate Limitï¼‰

11. **AC-2.2.11**: å•å…ƒæµ‹è¯•è¦†ç›–ç‡ â‰¥ 80%
    - æµ‹è¯•æ–‡ä»¶è¿‡æ»¤é€»è¾‘
    - æµ‹è¯•åˆ†å—ç­–ç•¥ï¼ˆMock AST è§£æï¼‰
    - æµ‹è¯• Embedding æ‰¹å¤„ç†ï¼ˆMock OpenAI APIï¼‰
    - æµ‹è¯• ChromaDB å­˜å‚¨ï¼ˆä½¿ç”¨å†…å­˜æ¨¡å¼ï¼‰

12. **AC-2.2.12**: ä»£ç ç¬¦åˆé¡¹ç›®è§„èŒƒ
    - éµå¾ª `coding-standards.md` ä¸­çš„ Python è§„èŒƒ
    - ä½¿ç”¨ Type Hints
    - ä½¿ç”¨ Pydantic è¿›è¡Œæ•°æ®éªŒè¯
    - ä½¿ç”¨ structlog è®°å½•ç»“æ„åŒ–æ—¥å¿—

---

## ğŸ”§ æŠ€æœ¯å®ç°ä»»åŠ¡

### Task 1: è®¾è®¡æ•°æ®æ¨¡å‹å’Œ Schemas
**é¢„ä¼°**: 30 åˆ†é’Ÿ

åœ¨ `backend/app/schemas/` ä¸­åˆ›å»ºæ•°æ®æ¨¡å‹ï¼š

```python
# backend/app/schemas/content.py
from typing import Literal, Optional
from pydantic import BaseModel, Field

class CodeChunk(BaseModel):
    """ä»£ç å—æ¨¡å‹"""
    content: str = Field(..., description="ä»£ç å†…å®¹")
    file_path: str = Field(..., description="æ–‡ä»¶è·¯å¾„")
    language: str = Field(..., description="ç¼–ç¨‹è¯­è¨€")
    chunk_type: Literal["file", "function", "class"] = Field(..., description="åˆ†å—ç±»å‹")
    start_line: int = Field(..., description="èµ·å§‹è¡Œå·")
    end_line: int = Field(..., description="ç»“æŸè¡Œå·")
    chunk_index: int = Field(0, description="åˆ†å—ç´¢å¼•")
    token_count: int = Field(..., description="Token æ•°é‡")

class VectorizeRequest(BaseModel):
    """å‘é‡åŒ–è¯·æ±‚"""
    repo_url: str = Field(..., description="ä»“åº“ URL")

class VectorizeStatus(BaseModel):
    """å‘é‡åŒ–çŠ¶æ€"""
    task_id: str
    status: Literal["pending", "processing", "completed", "failed"]
    progress: Optional[dict] = None  # {"processed": 50, "total": 100}
    error: Optional[str] = None
    stats: Optional[dict] = None  # {"chunks": 500, "files": 100, "vectors": 500}
```

---

### Task 2: å®ç°æ–‡ä»¶è¿‡æ»¤æœåŠ¡
**é¢„ä¼°**: 1 å°æ—¶

åˆ›å»º `backend/app/services/file_filter.py`ï¼š

```python
import os
from pathlib import Path
from typing import List, Set
import fnmatch

class FileFilter:
    """æ–‡ä»¶è¿‡æ»¤å™¨"""

    # é»˜è®¤å¿½ç•¥ç›®å½•
    IGNORE_DIRS = {
        "node_modules", ".git", "venv", "env", ".venv",
        "dist", "build", "__pycache__", ".next", ".nuxt",
        "target", "out", "coverage"
    }

    # é»˜è®¤å¿½ç•¥æ–‡ä»¶æ¨¡å¼
    IGNORE_PATTERNS = [
        "*.pyc", "*.pyo", "*.so", "*.dll", "*.dylib",
        "*.png", "*.jpg", "*.jpeg", "*.gif", "*.svg", "*.ico",
        "*.mp4", "*.avi", "*.mov",
        "*.zip", "*.tar", "*.gz", "*.rar",
        "package-lock.json", "yarn.lock", "pnpm-lock.yaml"
    ]

    # æ”¯æŒçš„ä»£ç æ–‡ä»¶æ‰©å±•å
    CODE_EXTENSIONS = {
        ".py", ".js", ".ts", ".jsx", ".tsx",
        ".java", ".go", ".rs", ".c", ".cpp", ".h", ".hpp",
        ".rb", ".php", ".swift", ".kt", ".scala",
        ".md", ".txt", ".yml", ".yaml", ".json", ".toml"
    }

    def __init__(self, repo_path: Path):
        self.repo_path = repo_path
        self.gitignore_patterns = self._load_gitignore()

    def _load_gitignore(self) -> List[str]:
        """åŠ è½½ .gitignore è§„åˆ™"""
        gitignore_path = self.repo_path / ".gitignore"
        if not gitignore_path.exists():
            return []

        patterns = []
        with open(gitignore_path, "r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith("#"):
                    patterns.append(line)
        return patterns

    def should_ignore_dir(self, dir_name: str) -> bool:
        """åˆ¤æ–­ç›®å½•æ˜¯å¦åº”è¯¥å¿½ç•¥"""
        if dir_name in self.IGNORE_DIRS:
            return True

        for pattern in self.gitignore_patterns:
            if fnmatch.fnmatch(dir_name, pattern.rstrip("/")):
                return True

        return False

    def should_ignore_file(self, file_path: Path) -> bool:
        """åˆ¤æ–­æ–‡ä»¶æ˜¯å¦åº”è¯¥å¿½ç•¥"""
        # æ£€æŸ¥æ‰©å±•å
        if file_path.suffix not in self.CODE_EXTENSIONS:
            return True

        # æ£€æŸ¥æ–‡ä»¶åæ¨¡å¼
        for pattern in self.IGNORE_PATTERNS:
            if fnmatch.fnmatch(file_path.name, pattern):
                return True

        # æ£€æŸ¥ gitignore
        relative_path = file_path.relative_to(self.repo_path)
        for pattern in self.gitignore_patterns:
            if fnmatch.fnmatch(str(relative_path), pattern):
                return True

        # æ£€æŸ¥æ–‡ä»¶å¤§å°
        if file_path.stat().st_size > 1024 * 1024:  # 1MB
            return True

        return False

    def get_code_files(self) -> List[Path]:
        """è·å–æ‰€æœ‰ä»£ç æ–‡ä»¶"""
        code_files = []

        for root, dirs, files in os.walk(self.repo_path):
            # è¿‡æ»¤ç›®å½•
            dirs[:] = [d for d in dirs if not self.should_ignore_dir(d)]

            # æ”¶é›†æ–‡ä»¶
            for file in files:
                file_path = Path(root) / file
                if not self.should_ignore_file(file_path):
                    code_files.append(file_path)

        return code_files
```

**æµ‹è¯•**: åˆ›å»º `backend/tests/test_file_filter.py`

---

### Task 3: å®ç°æ–‡æœ¬åˆ†å—æœåŠ¡
**é¢„ä¼°**: 2 å°æ—¶

åˆ›å»º `backend/app/services/chunker.py`ï¼š

```python
from typing import List
from pathlib import Path
import tiktoken
from app.schemas.content import CodeChunk

class CodeChunker:
    """ä»£ç åˆ†å—å™¨"""

    MAX_TOKENS = 2000  # åˆ†å—é˜ˆå€¼

    def __init__(self):
        self.encoder = tiktoken.get_encoding("cl100k_base")

    def count_tokens(self, text: str) -> int:
        """è®¡ç®— token æ•°é‡"""
        return len(self.encoder.encode(text))

    def chunk_file(self, file_path: Path, repo_root: Path) -> List[CodeChunk]:
        """å¯¹å•ä¸ªæ–‡ä»¶è¿›è¡Œåˆ†å—"""
        try:
            content = file_path.read_text(encoding="utf-8")
        except UnicodeDecodeError:
            # å°è¯•å…¶ä»–ç¼–ç 
            try:
                content = file_path.read_text(encoding="gbk")
            except Exception:
                return []  # è·³è¿‡æ— æ³•è¯»å–çš„æ–‡ä»¶

        token_count = self.count_tokens(content)
        relative_path = file_path.relative_to(repo_root)
        language = self._detect_language(file_path.suffix)

        # å°æ–‡ä»¶ï¼šæ•´æ–‡ä»¶ä½œä¸ºä¸€ä¸ª chunk
        if token_count < self.MAX_TOKENS:
            return [CodeChunk(
                content=content,
                file_path=str(relative_path),
                language=language,
                chunk_type="file",
                start_line=1,
                end_line=len(content.splitlines()),
                chunk_index=0,
                token_count=token_count
            )]

        # å¤§æ–‡ä»¶ï¼šå°è¯•æŒ‰å‡½æ•°/ç±»åˆ†å—
        chunks = self._chunk_by_syntax(content, relative_path, language)
        if chunks:
            return chunks

        # Fallback: æŒ‰è¡Œæ•°åˆ†å—
        return self._chunk_by_lines(content, relative_path, language)

    def _detect_language(self, suffix: str) -> str:
        """æ£€æµ‹ç¼–ç¨‹è¯­è¨€"""
        lang_map = {
            ".py": "python",
            ".js": "javascript",
            ".ts": "typescript",
            ".jsx": "javascript",
            ".tsx": "typescript",
            ".java": "java",
            ".go": "go",
            ".rs": "rust",
            ".c": "c",
            ".cpp": "cpp",
            ".h": "c",
            ".hpp": "cpp",
            ".rb": "ruby",
            ".php": "php",
            ".md": "markdown"
        }
        return lang_map.get(suffix, "unknown")

    def _chunk_by_syntax(
        self, content: str, file_path: Path, language: str
    ) -> List[CodeChunk]:
        """æŒ‰è¯­æ³•ç»“æ„åˆ†å—ï¼ˆç®€åŒ–ç‰ˆï¼ŒMVP é˜¶æ®µä½¿ç”¨ regexï¼‰"""
        # TODO: é›†æˆ tree-sitter è¿›è¡Œå‡†ç¡®çš„ AST è§£æ
        # å½“å‰ä½¿ç”¨ç®€å•çš„æ­£åˆ™è¡¨è¾¾å¼è¯†åˆ«å‡½æ•°/ç±»

        if language == "python":
            return self._chunk_python_simple(content, file_path)
        elif language in ["javascript", "typescript"]:
            return self._chunk_js_simple(content, file_path)

        return []  # ä¸æ”¯æŒçš„è¯­è¨€ï¼Œè¿”å›ç©ºåˆ—è¡¨ï¼ŒFallback åˆ°æŒ‰è¡Œåˆ†å—

    def _chunk_python_simple(self, content: str, file_path: Path) -> List[CodeChunk]:
        """ç®€å•çš„ Python åˆ†å—ï¼ˆåŸºäºç¼©è¿›ï¼‰"""
        import re

        lines = content.splitlines()
        chunks = []
        current_chunk = []
        start_line = 1
        chunk_index = 0

        for i, line in enumerate(lines, 1):
            # æ£€æµ‹é¡¶çº§å‡½æ•°æˆ–ç±»å®šä¹‰
            if re.match(r'^(def|class)\s+\w+', line):
                if current_chunk:
                    # ä¿å­˜ä¸Šä¸€ä¸ª chunk
                    chunk_content = "\n".join(current_chunk)
                    if self.count_tokens(chunk_content) > 0:
                        chunks.append(CodeChunk(
                            content=chunk_content,
                            file_path=str(file_path),
                            language="python",
                            chunk_type="function",
                            start_line=start_line,
                            end_line=i - 1,
                            chunk_index=chunk_index,
                            token_count=self.count_tokens(chunk_content)
                        ))
                        chunk_index += 1

                # å¼€å§‹æ–° chunk
                current_chunk = [line]
                start_line = i
            else:
                current_chunk.append(line)

        # ä¿å­˜æœ€åä¸€ä¸ª chunk
        if current_chunk:
            chunk_content = "\n".join(current_chunk)
            chunks.append(CodeChunk(
                content=chunk_content,
                file_path=str(file_path),
                language="python",
                chunk_type="function",
                start_line=start_line,
                end_line=len(lines),
                chunk_index=chunk_index,
                token_count=self.count_tokens(chunk_content)
            ))

        return chunks

    def _chunk_js_simple(self, content: str, file_path: Path) -> List[CodeChunk]:
        """ç®€å•çš„ JS/TS åˆ†å—"""
        # ç±»ä¼¼ Pythonï¼ŒåŸºäº function/class å…³é”®å­—
        # å®ç°ç•¥ï¼Œé€»è¾‘ç±»ä¼¼
        return []

    def _chunk_by_lines(
        self, content: str, file_path: Path, language: str
    ) -> List[CodeChunk]:
        """æŒ‰è¡Œæ•°åˆ†å—ï¼ˆFallbackï¼‰"""
        lines = content.splitlines()
        chunks = []
        chunk_size = 100  # æ¯ä¸ª chunk 100 è¡Œ

        for i in range(0, len(lines), chunk_size):
            chunk_lines = lines[i:i + chunk_size]
            chunk_content = "\n".join(chunk_lines)

            chunks.append(CodeChunk(
                content=chunk_content,
                file_path=str(file_path),
                language=language,
                chunk_type="file",
                start_line=i + 1,
                end_line=min(i + chunk_size, len(lines)),
                chunk_index=i // chunk_size,
                token_count=self.count_tokens(chunk_content)
            ))

        return chunks
```

**æµ‹è¯•**: åˆ›å»º `backend/tests/test_chunker.py`

---

### Task 4: å®ç° Embedding ç”ŸæˆæœåŠ¡
**é¢„ä¼°**: 1.5 å°æ—¶

åˆ›å»º `backend/app/services/embedder.py`ï¼š

```python
from typing import List
import asyncio
from openai import AsyncOpenAI
import structlog
from app.schemas.content import CodeChunk
from app.core.config import settings

logger = structlog.get_logger()

class EmbeddingService:
    """Embedding ç”ŸæˆæœåŠ¡"""

    MODEL = "text-embedding-ada-002"
    BATCH_SIZE = 100  # æ¯æ‰¹å¤„ç†æ•°é‡
    MAX_RETRIES = 3

    def __init__(self):
        self.client = AsyncOpenAI(api_key=settings.OPENAI_API_KEY)
        self.semaphore = asyncio.Semaphore(10)  # å¹¶å‘é™åˆ¶

    async def generate_embeddings(
        self, chunks: List[CodeChunk]
    ) -> List[List[float]]:
        """æ‰¹é‡ç”Ÿæˆ embeddings"""
        all_embeddings = []

        for i in range(0, len(chunks), self.BATCH_SIZE):
            batch = chunks[i:i + self.BATCH_SIZE]
            embeddings = await self._process_batch(batch)
            all_embeddings.extend(embeddings)

            logger.info(
                "embeddings_generated",
                batch_num=i // self.BATCH_SIZE + 1,
                batch_size=len(batch)
            )

        return all_embeddings

    async def _process_batch(self, chunks: List[CodeChunk]) -> List[List[float]]:
        """å¤„ç†å•ä¸ªæ‰¹æ¬¡"""
        texts = [chunk.content for chunk in chunks]

        for attempt in range(self.MAX_RETRIES):
            try:
                async with self.semaphore:
                    response = await self.client.embeddings.create(
                        model=self.MODEL,
                        input=texts
                    )

                return [item.embedding for item in response.data]

            except Exception as e:
                logger.warning(
                    "embedding_retry",
                    attempt=attempt + 1,
                    error=str(e)
                )

                if attempt == self.MAX_RETRIES - 1:
                    raise

                # æŒ‡æ•°é€€é¿
                await asyncio.sleep(2 ** attempt)

        return []
```

**é…ç½®**: åœ¨ `backend/app/core/config.py` æ·»åŠ :

```python
class Settings(BaseSettings):
    OPENAI_API_KEY: str

    class Config:
        env_file = ".env"

settings = Settings()
```

**æµ‹è¯•**: åˆ›å»º `backend/tests/test_embedder.py` (Mock OpenAI API)

---

### Task 5: å®ç° ChromaDB é›†æˆæœåŠ¡
**é¢„ä¼°**: 1.5 å°æ—¶

åˆ›å»º `backend/app/services/vector_store.py`ï¼š

```python
from typing import List
import chromadb
from chromadb.config import Settings as ChromaSettings
from pathlib import Path
import structlog
from app.schemas.content import CodeChunk

logger = structlog.get_logger()

class VectorStore:
    """å‘é‡å­˜å‚¨æœåŠ¡"""

    def __init__(self, persist_directory: str = "./data/chromadb"):
        self.persist_directory = Path(persist_directory)
        self.persist_directory.mkdir(parents=True, exist_ok=True)

        self.client = chromadb.Client(ChromaSettings(
            chroma_db_impl="duckdb+parquet",
            persist_directory=str(self.persist_directory)
        ))

    def get_or_create_collection(self, repo_owner: str, repo_name: str):
        """è·å–æˆ–åˆ›å»º collection"""
        collection_name = f"repo_{repo_owner}_{repo_name}".lower()
        # ChromaDB collection åç§°åªèƒ½åŒ…å«å­—æ¯ã€æ•°å­—ã€ä¸‹åˆ’çº¿å’Œè¿å­—ç¬¦
        collection_name = collection_name.replace("-", "_")

        return self.client.get_or_create_collection(
            name=collection_name,
            metadata={"repo_owner": repo_owner, "repo_name": repo_name}
        )

    async def store_chunks(
        self,
        repo_owner: str,
        repo_name: str,
        chunks: List[CodeChunk],
        embeddings: List[List[float]]
    ) -> None:
        """å­˜å‚¨ä»£ç å—å’Œå‘é‡"""
        collection = self.get_or_create_collection(repo_owner, repo_name)

        # å‡†å¤‡æ•°æ®
        ids = [f"{chunk.file_path}::{chunk.chunk_index}" for chunk in chunks]
        documents = [chunk.content for chunk in chunks]
        metadatas = [
            {
                "file_path": chunk.file_path,
                "language": chunk.language,
                "chunk_type": chunk.chunk_type,
                "start_line": chunk.start_line,
                "end_line": chunk.end_line,
                "chunk_index": chunk.chunk_index,
                "token_count": chunk.token_count
            }
            for chunk in chunks
        ]

        # æ‰¹é‡æ’å…¥ï¼ˆupsert ä¿è¯å¹‚ç­‰æ€§ï¼‰
        collection.upsert(
            ids=ids,
            documents=documents,
            embeddings=embeddings,
            metadatas=metadatas
        )

        logger.info(
            "chunks_stored",
            repo=f"{repo_owner}/{repo_name}",
            count=len(chunks)
        )

    def get_collection_stats(self, repo_owner: str, repo_name: str) -> dict:
        """è·å– collection ç»Ÿè®¡ä¿¡æ¯"""
        collection = self.get_or_create_collection(repo_owner, repo_name)
        count = collection.count()

        return {
            "total_chunks": count,
            "collection_name": collection.name
        }
```

**æµ‹è¯•**: åˆ›å»º `backend/tests/test_vector_store.py`

---

### Task 6: å®ç°å‘é‡åŒ–ç¼–æ’æœåŠ¡
**é¢„ä¼°**: 2 å°æ—¶

åˆ›å»º `backend/app/services/vectorization.py`ï¼š

```python
from typing import Dict
from pathlib import Path
import asyncio
import structlog
from app.services.file_filter import FileFilter
from app.services.chunker import CodeChunker
from app.services.embedder import EmbeddingService
from app.services.vector_store import VectorStore
from app.schemas.content import VectorizeStatus

logger = structlog.get_logger()

class VectorizationService:
    """å‘é‡åŒ–ç¼–æ’æœåŠ¡"""

    def __init__(self):
        self.chunker = CodeChunker()
        self.embedder = EmbeddingService()
        self.vector_store = VectorStore()
        self.tasks: Dict[str, VectorizeStatus] = {}

    async def vectorize_repository(
        self,
        task_id: str,
        repo_path: Path,
        repo_owner: str,
        repo_name: str
    ) -> None:
        """å‘é‡åŒ–æ•´ä¸ªä»“åº“"""
        try:
            # æ›´æ–°çŠ¶æ€
            self.tasks[task_id] = VectorizeStatus(
                task_id=task_id,
                status="processing",
                progress={"processed": 0, "total": 0}
            )

            # 1. è¿‡æ»¤æ–‡ä»¶
            logger.info("starting_file_filtering", repo=f"{repo_owner}/{repo_name}")
            file_filter = FileFilter(repo_path)
            code_files = file_filter.get_code_files()
            total_files = len(code_files)

            logger.info(
                "files_filtered",
                total=total_files,
                repo=f"{repo_owner}/{repo_name}"
            )

            # 2. åˆ†å—
            logger.info("starting_chunking")
            all_chunks = []
            for i, file_path in enumerate(code_files):
                chunks = self.chunker.chunk_file(file_path, repo_path)
                all_chunks.extend(chunks)

                # æ›´æ–°è¿›åº¦
                self.tasks[task_id].progress = {
                    "processed": i + 1,
                    "total": total_files
                }

            logger.info("chunking_completed", chunks=len(all_chunks))

            # 3. ç”Ÿæˆ embeddings
            logger.info("starting_embedding_generation")
            embeddings = await self.embedder.generate_embeddings(all_chunks)
            logger.info("embeddings_generated", count=len(embeddings))

            # 4. å­˜å‚¨åˆ° ChromaDB
            logger.info("storing_to_chromadb")
            await self.vector_store.store_chunks(
                repo_owner, repo_name, all_chunks, embeddings
            )

            # 5. æ›´æ–°çŠ¶æ€ä¸ºå®Œæˆ
            stats = self.vector_store.get_collection_stats(repo_owner, repo_name)
            self.tasks[task_id] = VectorizeStatus(
                task_id=task_id,
                status="completed",
                stats={
                    "files": total_files,
                    "chunks": len(all_chunks),
                    "vectors": len(embeddings),
                    **stats
                }
            )

            logger.info(
                "vectorization_completed",
                task_id=task_id,
                stats=self.tasks[task_id].stats
            )

        except Exception as e:
            logger.error(
                "vectorization_failed",
                task_id=task_id,
                error=str(e),
                exc_info=True
            )

            self.tasks[task_id] = VectorizeStatus(
                task_id=task_id,
                status="failed",
                error=str(e)
            )

    def get_task_status(self, task_id: str) -> VectorizeStatus:
        """è·å–ä»»åŠ¡çŠ¶æ€"""
        return self.tasks.get(
            task_id,
            VectorizeStatus(task_id=task_id, status="pending")
        )
```

**æµ‹è¯•**: åˆ›å»º `backend/tests/test_vectorization.py`

---

### Task 7: åˆ›å»º API è·¯ç”±
**é¢„ä¼°**: 1 å°æ—¶

åˆ›å»º `backend/app/api/routes/vectorize.py`ï¼š

```python
from fastapi import APIRouter, BackgroundTasks, HTTPException
from app.schemas.content import VectorizeRequest, VectorizeStatus
from app.services.vectorization import VectorizationService
from app.services.github_repo import GitHubRepoService
import uuid

router = APIRouter(prefix="/api/vectorize", tags=["vectorization"])

vectorization_service = VectorizationService()
github_service = GitHubRepoService()

@router.post("", response_model=dict)
async def start_vectorization(
    request: VectorizeRequest,
    background_tasks: BackgroundTasks
):
    """å¯åŠ¨å‘é‡åŒ–ä»»åŠ¡"""
    try:
        # è§£æä»“åº“ URL
        owner, repo_name = github_service.parse_repo_url(request.repo_url)

        # æ£€æŸ¥ä»“åº“æ˜¯å¦å·²å…‹éš†ï¼ˆå‡è®¾ Story 2.1 å·²å®Œæˆï¼‰
        repo_path = github_service.get_repo_path(owner, repo_name)
        if not repo_path.exists():
            raise HTTPException(
                status_code=404,
                detail=f"Repository not cloned: {owner}/{repo_name}"
            )

        # ç”Ÿæˆä»»åŠ¡ ID
        task_id = str(uuid.uuid4())

        # åœ¨åå°è¿è¡Œå‘é‡åŒ–
        background_tasks.add_task(
            vectorization_service.vectorize_repository,
            task_id,
            repo_path,
            owner,
            repo_name
        )

        return {
            "ok": True,
            "data": {
                "task_id": task_id,
                "status": "processing"
            }
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/{task_id}", response_model=dict)
async def get_vectorization_status(task_id: str):
    """è·å–å‘é‡åŒ–ä»»åŠ¡çŠ¶æ€"""
    status = vectorization_service.get_task_status(task_id)

    return {
        "ok": True,
        "data": status.dict()
    }
```

åœ¨ `backend/app/api/router.py` ä¸­æ³¨å†Œè·¯ç”±ï¼š

```python
from app.api.routes import vectorize

api_router.include_router(vectorize.router)
```

---

### Task 8: é…ç½® OpenAI API Key
**é¢„ä¼°**: 15 åˆ†é’Ÿ

æ›´æ–° `backend/.env.example`:

```env
OPENAI_API_KEY=sk-...
```

æ›´æ–° `backend/README.md` æ·»åŠ é…ç½®è¯´æ˜ã€‚

---

### Task 9: æ·»åŠ ä¾èµ–åŒ…
**é¢„ä¼°**: 15 åˆ†é’Ÿ

æ›´æ–° `backend/pyproject.toml`:

```toml
[tool.poetry.dependencies]
chromadb = "^0.4.18"
openai = "^1.3.0"
tiktoken = "^0.5.1"
```

è¿è¡Œ:
```bash
cd backend
poetry install
```

---

### Task 10: åˆ›å»ºå•å…ƒæµ‹è¯•
**é¢„ä¼°**: 3 å°æ—¶

åˆ›å»ºä»¥ä¸‹æµ‹è¯•æ–‡ä»¶ï¼š
- `backend/tests/test_file_filter.py`: æµ‹è¯•æ–‡ä»¶è¿‡æ»¤é€»è¾‘
- `backend/tests/test_chunker.py`: æµ‹è¯•åˆ†å—ç­–ç•¥
- `backend/tests/test_embedder.py`: Mock OpenAI API
- `backend/tests/test_vector_store.py`: ä½¿ç”¨å†…å­˜æ¨¡å¼ ChromaDB
- `backend/tests/test_vectorization.py`: é›†æˆæµ‹è¯•

è¿è¡Œæµ‹è¯•:
```bash
cd backend
poetry run pytest tests/ -v --cov=app --cov-report=term-missing
```

ç¡®ä¿è¦†ç›–ç‡ â‰¥ 80%ã€‚

---

### Task 11: é›†æˆæµ‹è¯•ï¼ˆç«¯åˆ°ç«¯ï¼‰
**é¢„ä¼°**: 1 å°æ—¶

åˆ›å»º `backend/tests/integration/test_vectorize_e2e.py`:

```python
import pytest
from fastapi.testclient import TestClient
from app.main import app

client = TestClient(app)

@pytest.mark.asyncio
async def test_vectorize_repository_e2e():
    """ç«¯åˆ°ç«¯æµ‹è¯•ï¼šå‘é‡åŒ–å®Œæ•´æµç¨‹"""
    # 1. å¯åŠ¨å‘é‡åŒ–
    response = client.post(
        "/api/vectorize",
        json={"repo_url": "https://github.com/octocat/Hello-World"}
    )
    assert response.status_code == 200
    data = response.json()
    assert data["ok"] is True
    task_id = data["data"]["task_id"]

    # 2. è½®è¯¢çŠ¶æ€ç›´åˆ°å®Œæˆ
    import time
    for _ in range(30):  # æœ€å¤šç­‰å¾… 30 ç§’
        response = client.get(f"/api/vectorize/{task_id}")
        status = response.json()["data"]["status"]

        if status == "completed":
            break
        elif status == "failed":
            pytest.fail("Vectorization failed")

        time.sleep(1)

    assert status == "completed"

    # 3. éªŒè¯ç»Ÿè®¡ä¿¡æ¯
    stats = response.json()["data"]["stats"]
    assert stats["files"] > 0
    assert stats["chunks"] > 0
    assert stats["vectors"] > 0
```

---

### Task 12: æ–‡æ¡£æ›´æ–°
**é¢„ä¼°**: 30 åˆ†é’Ÿ

æ›´æ–° `backend/README.md`:

```markdown
## å‘é‡åŒ–æœåŠ¡

### å¯åŠ¨å‘é‡åŒ–

**POST** `/api/vectorize`

Request:
\`\`\`json
{
  "repo_url": "https://github.com/facebook/react"
}
\`\`\`

Response:
\`\`\`json
{
  "ok": true,
  "data": {
    "task_id": "uuid...",
    "status": "processing"
  }
}
\`\`\`

### æŸ¥è¯¢çŠ¶æ€

**GET** `/api/vectorize/{task_id}`

Response:
\`\`\`json
{
  "ok": true,
  "data": {
    "task_id": "...",
    "status": "completed",
    "stats": {
      "files": 150,
      "chunks": 800,
      "vectors": 800
    }
  }
}
\`\`\`
```

---

## ğŸš¨ é£é™©ä¸ä¾èµ–

### é£é™©è¯„ä¼°

| é£é™© | æ¦‚ç‡ | å½±å“ | ç¼“è§£æªæ–½ |
|------|------|------|----------|
| OpenAI API Rate Limit | é«˜ | ä¸­ | å®ç°é‡è¯•æœºåˆ¶ã€å¹¶å‘æ§åˆ¶ã€ä½¿ç”¨ Batch API |
| å¤§æ–‡ä»¶å¤„ç†å†…å­˜æº¢å‡º | ä¸­ | é«˜ | æµå¼å¤„ç†ã€åˆ†æ‰¹åŠ è½½ã€è®¾ç½®æ–‡ä»¶å¤§å°é™åˆ¶ |
| ChromaDB å†™å…¥å†²çª | ä½ | ä¸­ | ä½¿ç”¨ upsertã€å®ç°å¹‚ç­‰æ€§ |
| AST è§£æå¤±è´¥ | ä¸­ | ä½ | Fallback åˆ°æŒ‰è¡Œåˆ†å— |
| OpenAI API è´¹ç”¨è¶…æ”¯ | ä¸­ | é«˜ | è®¾ç½®é…é¢é™åˆ¶ã€ç›‘æ§ä½¿ç”¨é‡ |

### ä¾èµ–å…³ç³»

**å‰ç½®ä¾èµ–**:
- STORY-2.1: GitHub ä»“åº“è§£ææœåŠ¡ï¼ˆéœ€è¦å…‹éš†çš„ä»“åº“è·¯å¾„ï¼‰

**åç»­ä¾èµ–**:
- STORY-2.3: RAG æ•™ç¨‹ç”ŸæˆæœåŠ¡ï¼ˆéœ€è¦å‘é‡æ•°æ®ï¼‰

---

## âœ… Definition of Done

- [ ] æ‰€æœ‰ 12 ä¸ªéªŒæ”¶æ ‡å‡†é€šè¿‡
- [ ] å•å…ƒæµ‹è¯•è¦†ç›–ç‡ â‰¥ 80%
- [ ] é›†æˆæµ‹è¯•é€šè¿‡ï¼ˆç«¯åˆ°ç«¯å‘é‡åŒ–æµç¨‹ï¼‰
- [ ] API æ–‡æ¡£æ›´æ–°
- [ ] ä»£ç ç¬¦åˆ `coding-standards.md` è§„èŒƒ
- [ ] Code Review é€šè¿‡
- [ ] åœ¨æœ¬åœ°ç¯å¢ƒæˆåŠŸå‘é‡åŒ–æµ‹è¯•ä»“åº“ï¼ˆå¦‚ `octocat/Hello-World`ï¼‰
- [ ] OpenAI API é”™è¯¯å¤„ç†æµ‹è¯•é€šè¿‡
- [ ] ChromaDB æ•°æ®æŒä¹…åŒ–éªŒè¯
- [ ] æ€§èƒ½æµ‹è¯•ï¼š1000 æ–‡ä»¶ â‰¤ 5 åˆ†é’Ÿ
- [ ] æ—¥å¿—è®°å½•å®Œå–„ï¼ŒåŒ…å«æ‰€æœ‰å…³é”®æ“ä½œ
- [ ] ä¾èµ–é¡¹å·²æ·»åŠ åˆ° `pyproject.toml`

---

## ğŸ“ Dev Agent Record

### å¼€å‘æ—¥å¿—

**æ—¶é—´**: YYYY-MM-DD
**å¼€å‘è€…**: Dev Agent

#### è¿›å±•
- [ ] Task 1: æ•°æ®æ¨¡å‹è®¾è®¡
- [ ] Task 2: æ–‡ä»¶è¿‡æ»¤æœåŠ¡
- [ ] Task 3: æ–‡æœ¬åˆ†å—æœåŠ¡
- [ ] Task 4: Embedding ç”Ÿæˆ
- [ ] Task 5: ChromaDB é›†æˆ
- [ ] Task 6: å‘é‡åŒ–ç¼–æ’
- [ ] Task 7: API è·¯ç”±
- [ ] Task 8-12: é…ç½®ã€æµ‹è¯•ã€æ–‡æ¡£

#### æŠ€æœ¯å†³ç­–
- åˆ†å—ç­–ç•¥ï¼šä½¿ç”¨ç®€åŒ–çš„æ­£åˆ™è¡¨è¾¾å¼ï¼ˆMVPï¼‰ï¼Œåç»­å¯å‡çº§åˆ° tree-sitter
- Embedding æ‰¹å¤„ç†ï¼šæ¯æ‰¹ 100 ä¸ª chunksï¼Œå¹³è¡¡æ€§èƒ½å’Œ API é™åˆ¶
- ChromaDBï¼šä½¿ç”¨ DuckDB + Parquet æ¨¡å¼ï¼Œæ”¯æŒæ•°æ®æŒä¹…åŒ–

#### é‡åˆ°çš„é—®é¢˜
_(è®°å½•å®é™…å¼€å‘ä¸­é‡åˆ°çš„é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ)_

#### æµ‹è¯•ç»“æœ
_(è®°å½•æµ‹è¯•è¦†ç›–ç‡å’Œå…³é”®æµ‹è¯•ç”¨ä¾‹)_

---

## ğŸ”— ç›¸å…³æ–‡æ¡£

- [Epic: MVP v0.1](./epic-mvp-v0.1.md)
- [Story 2.1: GitHub ä»“åº“è§£æ](./story-2.1-github-repo-service.md)
- [Story 2.3: RAG æ•™ç¨‹ç”Ÿæˆ](./story-2.3-rag-tutorial-generation.md) (å¾…åˆ›å»º)
- [æ¶æ„æ–‡æ¡£](../architecture.md)
- [æŠ€æœ¯æ ˆ](../architecture/tech-stack.md)
- [ç¼–ç è§„èŒƒ](../architecture/coding-standards.md)
- [ChromaDB æ–‡æ¡£](https://docs.trychroma.com/)
- [OpenAI Embeddings API](https://platform.openai.com/docs/guides/embeddings)
- [tiktoken æ–‡æ¡£](https://github.com/openai/tiktoken)
